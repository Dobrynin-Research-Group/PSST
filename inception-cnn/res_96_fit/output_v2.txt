nohup: ignoring input
device = device(type='cuda')
Epoch	train_loss	train_err[0]	train_err[1]	train_err[2]	test_loss	test_err[0]	test_err[1]	test_err[2]	time
/home/sayko/.local/lib/python3.8/site-packages/scipy/optimize/minpack.py:833: OptimizeWarning: Covariance of the parameters could not be estimated
  warnings.warn('Covariance of the parameters could not be estimated',
1	0.021583	1.8248	5.8591	3.7903	0.009691	1.2628	3.0587	0.6812	203.78
2	0.009058	1.4779	1.0042	5.5224	0.007119	0.9794	0.3096	0.7858	203.93
3	0.007502	0.5657	1.2282	0.9090	0.006878	12.5361	0.8694	0.6561	204.62
4	0.006829	0.6363	0.7266	1.2463	0.006181	0.3262	0.3780	0.7696	205.83
5	0.006336	0.4975	1.0527	0.6927	0.006101	0.3786	0.7856	0.6643	209.25
6	0.006080	0.4658	0.6464	0.6453	0.005901	0.4666	0.8670	0.4851	207.51
7	0.006036	0.4030	0.4542	1.4298	0.005750	0.3452	0.5912	0.6846	208.43
8	0.005927	0.5047	0.7186	0.6227	0.005685	0.4649	1.0327	0.5442	208.30
9	0.005819	0.4046	0.4844	0.6082	0.005728	0.5212	0.6392	0.5087	208.69
10	0.005774	0.5171	0.3944	0.6038	0.005417	0.5412	0.5651	0.5065	208.78
11	0.005650	0.5671	0.3994	0.5992	0.005442	0.2510	0.3367	0.5707	209.00
12	0.005577	0.4002	0.6232	0.5886	0.005516	0.5006	1.4573	0.4795	208.93
13	0.005581	0.3613	1.4111	0.5773	0.005204	0.2409	0.4931	0.6265	208.37
14	0.005398	0.3544	0.9851	0.5706	0.005592	0.3403	0.4573	0.6866	209.16
15	0.005367	0.4861	4.5723	0.5536	0.005455	0.7874	0.6304	0.4767	207.66
16	0.005418	0.5215	0.6756	0.5679	0.005221	0.3379	1.9202	0.4865	209.28
17	0.005339	2.3909	0.6995	0.5671	0.005285	0.3145	0.2546	0.5840	208.51
18	0.005369	0.4473	0.3725	0.5581	0.005219	3.3830	0.6288	0.4726	209.36
19	0.005394	0.3311	0.9721	0.5504	0.005026	0.2602	0.3159	0.4660	209.47
20	0.005321	0.8267	0.4835	0.5480	0.005847	0.4919	0.8251	0.5217	209.09
21	0.005304	0.5298	0.4203	0.5494	0.005640	0.5378	0.3221	0.4675	208.90
22	0.005300	0.3511	0.4458	0.5443	0.005640	0.3615	0.2959	0.5112	210.63
23	0.005201	0.3566	0.6035	0.5349	0.005457	0.3816	0.2664	0.4891	210.76
24	0.005221	0.3876	0.6821	0.5378	0.005028	0.7505	0.3951	0.4750	210.86
25	0.005243	0.4042	0.6877	0.5296	0.005322	0.2414	0.6420	0.6399	211.15
26	0.005260	0.4132	0.3622	0.5363	0.005285	0.5302	0.3996	0.5559	210.75
27	0.005138	0.3779	0.4681	0.5272	0.004786	0.3897	0.2745	0.4787	210.67
28	0.005115	0.3659	0.4743	0.5251	0.005038	0.4141	0.4677	0.4446	209.04
29	0.005251	0.3543	0.4071	0.5284	0.005041	0.3016	0.5269	0.4424	208.58
30	0.005126	0.4270	0.5503	0.5276	0.005174	0.4259	0.3286	0.6428	208.94
31	0.005131	0.3370	0.3733	0.5224	0.005039	2.6219	0.6771	0.4541	209.99
32	0.005197	0.3816	0.7480	0.6247	0.004952	0.4738	0.4417	0.5330	209.64
33	0.005174	0.5043	0.3813	0.5279	0.005172	0.2839	0.4673	0.5983	209.45
34	0.005156	0.3520	0.4967	0.5092	0.005271	0.2867	0.4752	0.5476	209.42
35	0.005145	0.5786	0.6588	0.5069	0.004883	0.3905	0.5532	0.4666	211.63
36	0.005189	0.4084	0.4604	0.5218	0.004888	1.0639	0.6466	0.4574	209.43
37	0.005053	0.4156	1.9898	0.5129	0.005053	0.3358	0.5898	0.4874	211.56
38	0.005030	0.4341	0.3683	0.5021	0.005385	1.7383	0.5640	0.7890	211.52
39	0.005036	0.6940	0.3972	0.5045	0.004944	0.4202	0.4130	0.4598	210.98
40	0.005039	0.4076	0.3572	0.5084	0.005132	0.6307	3.4094	0.4290	212.36
41	0.005089	0.3561	0.3627	0.5130	0.004962	0.2992	0.3164	0.4862	212.73
42	0.005056	0.3150	0.3372	0.5121	0.005339	0.3042	0.2296	0.5297	212.42
43	0.005001	0.4369	0.3657	0.5003	0.005199	0.4043	0.2158	0.5084	209.66
44	0.004972	0.7773	0.3756	0.4957	0.005097	0.2977	1.0311	0.4363	212.04
45	0.005022	0.3221	0.4941	0.5037	0.005134	0.3723	0.3575	0.5078	209.91
46	0.005052	0.3678	0.3904	0.5050	0.005015	0.6405	0.4993	0.4337	211.79
47	0.004980	0.5583	0.4076	0.4989	0.004805	0.2372	1.0539	0.4814	209.25
48	0.004944	0.3289	0.5629	0.4973	0.004945	0.3145	0.4178	0.4782	211.04
49	0.004900	0.4537	0.4802	0.4956	0.004879	0.2921	0.2800	0.6346	208.84
50	0.005040	0.8188	2.0107	0.4940	0.005028	0.4777	6.0568	0.4457	210.40
51	0.004964	0.9950	0.3373	0.4907	0.004738	0.2273	0.7177	0.4743	209.36
52	0.004973	0.6482	0.3657	0.4975	0.004804	0.4471	0.3544	0.4764	211.48
53	0.004996	0.5154	0.4029	0.5008	0.004765	0.3721	0.4038	0.4487	210.49
54	0.004973	0.3666	1.2357	0.4941	0.004793	0.2853	0.5299	0.4069	213.39
55	0.004965	0.3726	0.3961	0.4922	0.005002	0.8603	0.7469	0.4353	211.75
56	0.004888	0.3217	0.4092	0.4898	0.004868	0.2699	0.7579	0.4210	213.11
57	0.004940	0.4110	0.2917	0.5010	0.005429	19.8380	5.0303	0.4328	210.49
58	0.004919	0.3628	0.3207	0.4924	0.005094	0.6262	0.5734	0.4845	213.36
59	0.004990	0.4231	0.3867	0.4878	0.005340	0.3140	0.4782	0.5178	211.90
60	0.004948	0.5912	0.9889	0.4942	0.004743	0.2975	0.5988	0.4196	211.52
61	0.004957	0.3270	0.4005	0.4875	0.004782	0.5246	0.2629	0.4343	209.83
62	0.004909	0.3394	0.3522	0.4928	0.005092	0.2337	0.2497	0.5324	210.51
63	0.004987	0.2933	0.7240	0.4846	0.004948	0.4576	inf	0.4776	208.61
64	0.004910	0.3697	0.5920	0.4868	0.004962	0.4820	0.5310	0.4798	209.84
65	0.004868	0.3022	0.3904	0.4809	0.005128	0.2687	0.2277	0.5366	209.50
66	0.004890	0.4519	0.3549	0.6240	0.004832	0.3573	0.3200	0.4455	213.36
67	0.004831	1.4065	0.3587	0.4898	0.004943	8.2270	0.2958	0.4285	211.96
68	0.004891	0.3240	0.2963	0.4826	0.005154	0.2615	0.2255	0.5908	213.31
69	0.004860	0.7081	0.3896	0.4832	0.004791	0.2932	0.4964	0.4840	211.78
70	0.004837	0.4505	0.3812	0.4773	0.004863	0.4713	0.2375	0.4983	212.16
71	0.004835	0.3377	0.3982	0.5198	0.004850	0.4016	1.3705	0.4770	211.02
72	0.004920	1.5271	0.5804	0.4845	0.005031	0.5683	0.3904	0.7389	213.31
73	0.005017	1.1346	inf	0.5001	0.004855	0.4002	0.2243	0.5475	210.66
74	0.004903	0.3271	0.3527	0.4793	0.004675	0.4470	0.3063	0.4213	212.98
75	0.004915	0.3762	0.7506	0.4805	0.004734	0.3651	0.5850	0.4325	211.17
76	0.004875	0.4888	0.5404	0.4753	0.004799	0.4055	0.3987	0.4374	213.41
77	0.004862	0.3949	0.3608	0.4749	0.005238	0.2566	0.3714	0.5439	211.13
78	0.004792	0.3466	0.3357	0.4768	0.004880	0.2920	0.3989	0.4505	210.38
79	0.004814	0.4288	0.3923	0.4750	0.004960	0.5670	0.6572	0.4420	209.34
80	0.004921	0.3445	0.3867	0.4800	0.004730	0.3437	0.2037	0.4592	209.65
81	0.004786	0.3639	0.4204	0.4753	0.004862	0.4477	0.2871	0.4759	210.14
82	0.004872	0.3232	0.6400	0.4782	0.004821	0.3910	0.3711	0.4300	212.75
83	0.004829	0.3437	0.6356	0.4704	0.004734	0.3474	0.3820	0.4384	212.10
84	0.004765	0.3692	0.5149	0.4760	0.004711	0.5793	0.3460	0.4581	213.13
85	0.004784	0.4421	0.4246	0.4674	0.004595	0.5363	0.4187	0.4275	210.49
86	0.004707	0.3622	0.4083	0.4665	0.004838	0.2545	0.3159	0.6218	211.87
87	0.004841	0.4620	0.3704	0.4729	0.004861	0.2943	0.3868	0.4583	211.32
88	0.004803	0.5054	0.3834	0.4657	0.004664	0.2293	0.3528	0.4880	210.59
89	0.004797	0.3271	0.4611	0.4749	0.004760	0.2941	0.4232	0.4507	210.51
90	0.004823	0.3713	0.8002	0.4714	0.004844	0.3097	0.4665	0.5118	211.51
91	0.004752	0.3690	0.4039	0.4723	0.004658	0.4618	0.3349	0.4102	209.23
92	0.004728	0.3069	0.2887	0.4682	0.004500	1.1356	0.3856	0.4302	211.44
93	0.004830	0.3521	0.4345	0.4682	0.004676	0.2814	0.3251	0.4174	210.47
94	0.004774	0.4049	0.5990	0.4662	0.004919	0.5370	1.0168	0.4637	210.46
95	0.004794	0.3967	0.5953	0.4699	0.005237	0.3799	0.2295	0.6870	208.91
96	0.004827	0.7061	0.3687	0.4717	0.004907	0.5508	0.3534	0.4191	210.42
97	0.004880	0.2993	0.6494	0.4713	0.004648	0.3058	0.6253	0.4250	208.70
98	0.004772	0.3416	0.3841	0.4659	0.004626	0.4359	0.1965	0.4332	209.87
99	0.004724	0.4094	0.3100	0.4732	0.004897	0.3261	0.4349	0.4369	210.84
100	0.004718	0.3012	2.6818	0.4635	0.004837	0.4370	0.5025	0.4258	210.07
epoch_best_accuracy = 41
epoch_best_loss = 92
